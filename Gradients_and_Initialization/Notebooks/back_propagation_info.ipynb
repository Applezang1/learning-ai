{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb4525c-b859-4521-9c1a-8886f4f5a64d",
   "metadata": {},
   "source": [
    "# Back Propagation \n",
    "This notebook aims to help gain a mathematically understanding of back propagation by exploring forward and backward passes and how to compute each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc51b4-9a18-40fc-a642-2605c7f21fc8",
   "metadata": {},
   "source": [
    "## Example Model \n",
    "For this backpropagation, define a model with eight parameters whose loss is determined by the least squares loss function\n",
    "\n",
    "Define the model as:\n",
    "$$\n",
    "f[x,\\phi] = \\beta_3 + \\omega_3 \\cdot \\cos\\!\\left[\n",
    "\\beta_2 + \\omega_2 \\cdot \\exp\\!\\left(\n",
    "\\beta_1 + \\omega_1 \\cdot \\sin(\\beta_0 + \\omega_0 \\cdot x)\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Define the least squares loss function as:\n",
    "$$\n",
    "L[\\phi] = \\sum_i \\ell_i,\n",
    "\\qquad\n",
    "\\ell_i = \\left( f[x_i,\\phi] - y_i \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfd849-b89d-4d4d-8c8e-25fc9a5612ff",
   "metadata": {},
   "source": [
    "## Forward Pass \n",
    "In order to effectively compute the backward pass of the model, we need to first compute the forward pass. By computing the forward pass, we are able to compute and store the value and equation of the intermediate variables (pre-activation, activation/hidden unit output)\n",
    "\n",
    "Define the equations for the pre-activation and activation of this model as: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_0 &= \\beta_0 + \\omega_0 \\cdot x_i \\\\\n",
    "h_1 &= \\sin(f_0) \\\\\n",
    "f_1 &= \\beta_1 + \\omega_1 \\cdot h_1 \\\\\n",
    "h_2 &= \\exp(f_1) \\\\\n",
    "f_2 &= \\beta_2 + \\omega_2 \\cdot h_2 \\\\\n",
    "h_3 &= \\cos(f_2) \\\\\n",
    "f_3 &= \\beta_3 + \\omega_3 \\cdot h_3 \\\\\n",
    "\\ell_i &= (f_3 - y_i)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $f$ represents the pre-activation equations and $h$ represents the activation/hidden unit output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce913256-e837-4c23-848c-d51ae5db3c41",
   "metadata": {},
   "source": [
    "## Backward Pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d556b-c0a7-490e-b731-164ed80e923a",
   "metadata": {},
   "source": [
    "### Backward Pass #1 \n",
    "Backward Pass #1 computes the derivative of the loss function (least squares loss function in this case) in respect to each of the pre-activation/activation. These derivatives are represented as:\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial f_3},\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial h_3},\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial f_2},\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial h_2},\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial f_1},\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial h_1},\\quad\n",
    "\\text{and}\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial f_0}.\n",
    "$$ \n",
    "\n",
    "In order to compute the derivatives, we start backwards with the farthest pre-activation function. Therfore, we take theh derivative of the loss function in respect to the last pre-activation function ($f_3$). This is represented as:\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial f_3}\n",
    "= 2 (f_3 - y_i).\n",
    "$$ \n",
    "\n",
    "Next, we move up the list and compute the derivative of the loss function in respect to $h_3$. This is represented as: \n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial h_3}\n",
    "= \\frac{\\partial f_3}{\\partial h_3}\n",
    "  \\frac{\\partial \\ell_i}{\\partial f_3}.\n",
    "$$\n",
    "\n",
    "In order to compute the derivative, we needed to multiply $\\frac{\\partial \\ell_i}{\\partial f_3}$ by $\\frac{\\partial f_3}{\\partial h_3}$. This can be thought as an application of chain rule since: \n",
    "\n",
    "1) If the pre-activation ($f_3$) depends on the activation function ($h_3$)\n",
    "    \n",
    "2) The loss function ($\\ell_i$) depends on the pre-activation ($f_3$)\n",
    "    \n",
    "We can simplify this to a statement that the loss function ($\\ell_i$) depends on the activation function ($h_3$)\n",
    "\n",
    "<ins>Result</ins>: This same chain rule idea is repeated through all the pre-activation and activation functions until we have computed the derivative for the loss function ($\\ell_i$) in respect to all the activation and pre-activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb573b0-fbf7-4d35-a74f-9623b51137b7",
   "metadata": {},
   "source": [
    "### Backward Pass #2 \n",
    "Backward Pass #2 computes the derivative of the loss function (least squares loss function in this case) in respect to each parameter by using the computed derivatives from backward pass #1. \n",
    "\n",
    "For parameters ($\\beta_k$, $\\omega_k$), the derivatives are represented as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial \\beta_k} \n",
    "= \n",
    "\\frac{\\partial f_k}{\\partial \\beta_k} \n",
    "\\frac{\\partial \\ell_i}{\\partial f_k}\n",
    "\\qquad\n",
    "\\frac{\\partial \\ell_i}{\\partial \\omega_k} \n",
    "= \n",
    "\\frac{\\partial f_k}{\\partial \\omega_k} \n",
    "\\frac{\\partial \\ell_i}{\\partial f_k}\n",
    "$$ \n",
    "\n",
    "In order to compute these derivatives, we apply chain rule again since: \n",
    "\n",
    "1) If the pre-activation ($f_k$) depends on the parameter value ($\\beta_k$)\n",
    "   \n",
    "2) The loss function ($\\ell_i$) depends on the pre-activation ($f_k$)\n",
    "\n",
    "We can simplify this to a statement that the loss function ($\\ell_i$) depends on the parameter value ($\\beta_k$). The same reasoning process can be said for the $\\omega_k$. \n",
    "\n",
    "<ins>Result</ins>: The same chain rule idea is repeated through all the pre-activation functions until we have computed the derivatives for the loss function ($\\ell_i$) in respect all the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7bdc2-625c-466e-a03c-08a0ecf16adf",
   "metadata": {},
   "source": [
    "## Indicator Functions\n",
    "Indicator functions are binary filtering functions that control the output based on a condition of the indicator function. Indicator functions determine which activations contribute to the overall gradient. \n",
    "\n",
    "For example: \n",
    "\n",
    "- An indicator function might only allow the gradient to be computed if the activation function has a positive value. If the activation function is zero, the indicator function makes the gradient zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
