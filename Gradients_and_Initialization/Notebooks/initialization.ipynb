{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25080e6a-5c0c-4f13-87a1-78f2a2d22954",
   "metadata": {},
   "source": [
    "# Initialization \n",
    "This notebook aims to help gain a better understanding of indicator functions by defining a neural network and training the neural network using a backward pass, which involves using an indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78097248-bfcc-4142-adf8-ace37c587b49",
   "metadata": {},
   "source": [
    "### Imports \n",
    "Import the libraries needed to define the indicator function, neural network, and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2d3e64-9ede-4912-a71b-f18bb7e5ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d0b38-4985-4467-a0ce-889226e929a9",
   "metadata": {},
   "source": [
    "### Define Parameter Function\n",
    "Define a function that initializes the value of the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9964b121-aba6-4da9-8bf9-642b80339842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(K, D, sigma_sq_omega):\n",
    "  # Set seed in order to consistently obtain the same random number\n",
    "  np.random.seed(0)\n",
    "\n",
    "  # Input layer\n",
    "  D_i = 1\n",
    "  # Output layer\n",
    "  D_o = 1\n",
    "\n",
    "  # Construct an empty list for the weights and biases\n",
    "  all_weights = [None] * (K+1)\n",
    "  all_biases = [None] * (K+1)\n",
    "\n",
    "  # Construct an array structure and intialize the weights and biases of the input and output layers. Add varaince to the weights\n",
    "  all_weights[0] = np.random.normal(size=(D, D_i))*np.sqrt(sigma_sq_omega)\n",
    "  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n",
    "  all_biases[0] = np.zeros((D,1))\n",
    "  all_biases[-1]= np.zeros((D_o,1))\n",
    "\n",
    "  # Construct intermediate hidden layers\n",
    "  for layer in range(1,K):\n",
    "    all_weights[layer] = np.random.normal(size=(D,D))*np.sqrt(sigma_sq_omega)\n",
    "    all_biases[layer] = np.zeros((D,1))\n",
    "\n",
    "  return all_weights, all_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f1953-733c-4af5-982e-684457422445",
   "metadata": {},
   "source": [
    "### Define the Rectified Linear Unit (ReLU) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bf59d7-de27-45f8-a799-d7cd16b6cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954f77e-7139-40ba-848d-e80ade641ede",
   "metadata": {},
   "source": [
    "### Define Neural Network\n",
    "Define a function that computes a neural network given the value of the input, weights, and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b94dc33-1623-4171-a644-cb468308c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_output(net_input, all_weights, all_biases):\n",
    "\n",
    "  # Retrieve number of layers\n",
    "  K = len(all_weights)-1\n",
    "\n",
    "  # Store the pre-activations (all_f) and the activations (all_h)\n",
    "  all_f = [None] * (K+1)\n",
    "  all_h = [None] * (K+1)\n",
    "\n",
    "  all_h[0] = net_input\n",
    "\n",
    "  # Calculate the pre-activation and activation for each hidden layer\n",
    "  for layer in range(K):\n",
    "      # Compute the pre-activation function\n",
    "      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n",
    "      # Compute the activation function\n",
    "      all_h[layer+1] = ReLU(all_f[layer])\n",
    "\n",
    "  # Compute the output of the neural network from the last hidden layer\n",
    "  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n",
    "\n",
    "  # Retrieve the output\n",
    "  net_output = all_f[K]\n",
    "\n",
    "  return net_output, all_f, all_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85535213-c79c-44fc-aaa0-d78f09bf75a9",
   "metadata": {},
   "source": [
    "### Define Hyperparameters \n",
    "Define the hyperparameters (number of layers, number of neurons per layer, number of input layers, number of output layers) for the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab401426-4d05-4c83-9306-b194d14e3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of layers\n",
    "K = 5\n",
    "# Define the number of neurons per layer\n",
    "D = 8\n",
    "# Define the number of input layers\n",
    "D_i = 1\n",
    "# Define the number of output layer\n",
    "D_o = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a14e0-2674-4f37-85af-09fb089c637a",
   "metadata": {},
   "source": [
    "### Define Variance \n",
    "Define the variance of the initial weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47fdec7-d0a6-41df-8222-6cf2684a5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_sq_omega = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea5074-3e08-46a5-a821-9f9d9f3c8faa",
   "metadata": {},
   "source": [
    "### Initialize Neural Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e71b50-60d9-498a-9e91-432a68c21761",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fccfc0-9302-4953-ab7a-23210667a1f0",
   "metadata": {},
   "source": [
    "### Compute Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd420701-6b47-4e0f-99ab-662bb33d0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 1000\n",
    "data_in = np.random.normal(size=(1,n_data))\n",
    "net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974063d-2e56-40e0-be60-38e85308efb6",
   "metadata": {},
   "source": [
    "### Compute the Standard Deviation of the Hidden Units in each Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d7d0c9-b7e6-4b05-99c7-dd5c3c73c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1, std of hidden units = 0.811\n",
      "Layer 2, std of hidden units = 1.472\n",
      "Layer 3, std of hidden units = 4.547\n",
      "Layer 4, std of hidden units = 8.896\n",
      "Layer 5, std of hidden units = 10.106\n"
     ]
    }
   ],
   "source": [
    "for layer in range(1,K+1):\n",
    "  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56ade6-9a7c-47ab-a3c0-e0ab4a339640",
   "metadata": {},
   "source": [
    "### Define the Least Squares Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9559dc29-43fd-450e-b11f-fbd62bc6beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_loss(net_output, y):\n",
    "  return np.sum((net_output-y) * (net_output-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17433ac5-db9f-4f34-a42a-2e3a7701138d",
   "metadata": {},
   "source": [
    "### Define Derivative of Loss with Respect to Output\n",
    "Define a function that computes the derivative of the loss function with respect to the output of the neural network for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ba73b8-a924-4add-9478-a51cd5ee9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss_d_output(net_output, y):\n",
    "    return 2*(net_output -y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85568c90-6eaf-4e24-a2dc-ce405db96fe0",
   "metadata": {},
   "source": [
    "### Define an Indicator Function\n",
    "Define an indicator function for the backward pass that returns a 1 if the input is greater than equal to 0 and a 0 if the input is less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b3bb67-d14a-4647-93ca-383c7d802813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_function(x):\n",
    "  x_in = np.array(x)\n",
    "  x_in[x_in>=0] = 1\n",
    "  x_in[x_in<0] = 0\n",
    "  return x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95518477-1042-48d7-b116-31a835c3e021",
   "metadata": {},
   "source": [
    "### Define Backward Pass\n",
    "Define a function to compute the main backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8adf0b-3792-4d2f-b89d-f713bacfc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
    "  # Retrieve number of layers\n",
    "  K = len(all_weights) - 1\n",
    "\n",
    "  # Store the derivatives dl_dweights and dl_dbiases in lists\n",
    "  all_dl_dweights = [None] * (K+1)\n",
    "  all_dl_dbiases = [None] * (K+1)\n",
    "  # Store the derivatives of the loss with respect to the activation and preactivations in lists\n",
    "  all_dl_df = [None] * (K+1)\n",
    "  all_dl_dh = [None] * (K+1)\n",
    "\n",
    "  # Compute derivatives of the loss with respect to the network output\n",
    "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
    "\n",
    "  # Compute the backward pass\n",
    "  for layer in range(K,-1,-1):\n",
    "    # Calculate the derivatives of the loss with respect to the biases at layer\n",
    "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
    "    # Calculate the derivatives of the loss with respect to the weights at layer\n",
    "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].transpose())\n",
    "\n",
    "    # Calculate the derivatives of the loss with respect to the activations\n",
    "    all_dl_dh[layer] = np.matmul(all_weights[layer].transpose(), all_dl_df[layer])\n",
    "   \n",
    "    if layer > 0:\n",
    "       # Calculate the derivatives of the loss with respect to the pre-activation f\n",
    "      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n",
    "\n",
    "  return all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e84215-07a5-482e-9b53-eca66f333139",
   "metadata": {},
   "source": [
    "### Define Hyperparameters \n",
    "Define the hyperparameters (number of layers, number of neurons per layer, number of input layers, number of output layers) for the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f66a60d-d8f1-4ea9-9cd0-fc58dcade1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of layers\n",
    "K = 5\n",
    "# Define the number of neurons per layer\n",
    "D = 8\n",
    "# Define input layer\n",
    "D_i = 1\n",
    "# Define output layer\n",
    "D_o = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6edc13-6bdd-42e9-bd3d-741ea69574a1",
   "metadata": {},
   "source": [
    "### Define Variance \n",
    "Define the variance of the initial weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "172093a3-2f46-4f4e-84d0-5f33fa5548a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_sq_omega = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbee7bf-323a-4702-8007-e1fb18f2a20d",
   "metadata": {},
   "source": [
    "### Initialize Neural Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add6c331-7b5b-40e7-a285-b8ed8a5165a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights, all_biases = init_params(K,D,sigma_sq_omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9304631-54fc-49af-aacf-82b0711dfca8",
   "metadata": {},
   "source": [
    "### Initialize Gradient Array Structure\n",
    "Initialize an array structure, which stores the computed gradients of the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d1d7795-0158-442c-ba20-04b595b303f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 100\n",
    "aggregate_dl_df = [None] * (K+1)\n",
    "for layer in range(1,K):\n",
    "  # Store the gradients for every data point in the 3D array\n",
    "  aggregate_dl_df[layer] = np.zeros((D,n_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e6e31-9d84-410b-8caf-fb9ad6a9b1cf",
   "metadata": {},
   "source": [
    "### Compute Gradients \n",
    "Compute the derivatives of the parameters for each data point separately (gradient) and store it in the intiaialized gradient array structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd4ff39-f80c-4d77-aed9-fb84726e3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_data in range(n_data):\n",
    "  data_in = np.random.normal(size=(1,1))\n",
    "  y = np.zeros((1,1))\n",
    "  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n",
    "  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
    "  for layer in range(1,K):\n",
    "    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62384e-bfb0-4859-a80c-8261d495b498",
   "metadata": {},
   "source": [
    "### Compute the Standard Deviation of the Gradient for each Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27bfa517-f8ab-4e4c-8d4a-2a311625a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4, std of dl_dh = 56.472\n",
      "Layer 3, std of dl_dh = 109.132\n",
      "Layer 2, std of dl_dh = 340.657\n",
      "Layer 1, std of dl_dh = 446.654\n"
     ]
    }
   ],
   "source": [
    "for layer in reversed(range(1,K)):\n",
    "  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
