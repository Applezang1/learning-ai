{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d163a860-4dca-48fd-920a-6ac5691f9d01",
   "metadata": {},
   "source": [
    "# Graph Attention\n",
    "This notebook aims to help gain a better understanding of graph attention by defining a graph attention network and executing the graph attention network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351072e-1e1d-458f-b673-cfa336672024",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47af80e3-0fef-46c0-a326-44029cc835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae3585-fcd1-4bad-b704-e64a6f4489e5",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Initialize the hyperparameters of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64137cc4-7ba7-44e5-be14-217f505d1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Define the number of nodes in the graph\n",
    "N = 8\n",
    "# Define the number of dimensions of each input\n",
    "D = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86e44d-b1a6-4b20-a4af-e83d0ad0bb9d",
   "metadata": {},
   "source": [
    "### Define Adjacency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148921ac-c1e1-4eec-a325-ced74b5fa69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,1,0,1,0,0,0,0],\n",
    "              [1,0,1,1,1,0,0,0],\n",
    "              [0,1,0,0,1,0,0,0],\n",
    "              [1,1,0,0,1,0,0,0],\n",
    "              [0,1,1,1,0,1,0,1],\n",
    "              [0,0,0,0,1,0,1,1],\n",
    "              [0,0,0,0,0,1,0,0],\n",
    "              [0,0,0,0,1,1,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3dc47-7bf5-430d-92c0-d571ed9feee2",
   "metadata": {},
   "source": [
    "### Print Adjacency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba590731-cf13-43a5-b208-c5e9ff0cd534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 0 0]\n",
      " [1 0 1 1 1 0 0 0]\n",
      " [0 1 0 0 1 0 0 0]\n",
      " [1 1 0 0 1 0 0 0]\n",
      " [0 1 1 1 0 1 0 1]\n",
      " [0 0 0 0 1 0 1 1]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aac645-dd21-4127-816b-42047bb48fb8",
   "metadata": {},
   "source": [
    "### Define Input Data\n",
    "Define a matrix of random values as the input data for the graph attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a988c731-dc5b-4007-85cc-f6eb8845b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(D,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8abe2-5019-4c61-a833-52e4bf6aa07c",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Initialize random matrices for the parameters of the graph attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d50740f-4607-4e93-a963-64339041f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = np.random.normal(size=(D,D))\n",
    "beta = np.random.normal(size=(D,1))\n",
    "phi = np.random.normal(size=(2*D,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8a844-c70c-4fdc-b1bc-43539f2e5400",
   "metadata": {},
   "source": [
    "### Define Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830fe306-66e7-4491-a3ed-2c0fbfe368b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cols(data_in):\n",
    "  # Exponentiate the input value\n",
    "  exp_values = np.exp(data_in) \n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0)\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d20fd-c49d-4f57-bf45-542a4c81eadc",
   "metadata": {},
   "source": [
    "### Define the Rectified Linear Unit (ReLU) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0365e5-83ae-456b-b2e7-d8efa7e20c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e632e6-8581-4e5b-b34d-0aad5e5a1ca7",
   "metadata": {},
   "source": [
    "### Define the Graph Attention Network\n",
    "Define the graph attention network, which linearly transforms the input data, computes attention weights of pair of the transformed input data, and applies the attention weights to the final output of the graph attention network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9df777c-3f07-44fa-94d2-15b016e644e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_attention(X,omega, beta, phi, A):\n",
    "    # Apply a linear transformation to the input data\n",
    "    X_prime = beta + omega @ X   \n",
    "\n",
    "    # Compute all the pairs of the node embeddings\n",
    "    X_i = np.repeat(X_prime[:, :, np.newaxis], N, axis=2)   \n",
    "    X_j = np.repeat(X_prime[:, np.newaxis, :], N, axis=1)   \n",
    "    X_pair = np.concatenate([X_i, X_j], axis=0)             \n",
    "\n",
    "    # Compute matrix S, which represents the similarity of every node to every other\n",
    "    S = (phi.T @ X_pair.reshape(2*D, -1)).reshape(N, N)     \n",
    "    S = ReLU(S) \n",
    "\n",
    "    # Mask matrix S to make the attention weights of non-neighboring nodes to zero\n",
    "    A_hat = A + np.eye(N)\n",
    "    S_masked = np.where(A_hat==0, -1e20, S)\n",
    "    softmax_S = softmax_cols(S_masked)\n",
    "\n",
    "    # Apply the attention weights to the final output\n",
    "    output = ReLU(X_prime @ softmax_S)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fa4d5-e9be-4657-b34f-8d4c9b2edc32",
   "metadata": {},
   "source": [
    "### Print Output\n",
    "Print the output of the graph attention network and compare it with the correct answer to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecf077c-fe0d-4195-aecf-87629db5cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer is:\n",
      "[[0.    0.028 0.37  0.    0.97  0.    0.    0.698]\n",
      " [0.    0.    0.    0.    1.184 0.    2.654 0.  ]\n",
      " [1.13  0.564 0.    1.298 0.268 0.    0.    0.779]\n",
      " [0.825 0.    0.    1.175 0.    0.    0.    0.  ]]]\n",
      "Computed answer is:\n",
      "[[0.    0.028 0.37  0.    0.97  0.    0.    0.698]\n",
      " [0.    0.    0.    0.    1.184 0.    2.654 0.   ]\n",
      " [1.13  0.564 0.    1.298 0.268 0.    0.    0.779]\n",
      " [0.825 0.    0.    1.175 0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "output = graph_attention(X, omega, beta, phi, A)\n",
    "print(\"Correct answer is:\")\n",
    "print(\"[[0.    0.028 0.37  0.    0.97  0.    0.    0.698]\")\n",
    "print(\" [0.    0.    0.    0.    1.184 0.    2.654 0.  ]\")\n",
    "print(\" [1.13  0.564 0.    1.298 0.268 0.    0.    0.779]\")\n",
    "print(\" [0.825 0.    0.    1.175 0.    0.    0.    0.  ]]]\")\n",
    "print(\"Computed answer is:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
