{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025cecd6-4ccc-4e23-9f7c-359b52f3c516",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "This notebook aims to help gain a better understanding of batch normalization and defining a residual neural network model without batch normalization and with batch normalization and comparing the variance of the hidden unit outputs for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d417f4-99d8-44a7-9ae2-169136155787",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03786cce-c5d6-4592-896c-710eeaf9ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c6c7d-bef5-4bb6-9df5-c6c06188ed8a",
   "metadata": {},
   "source": [
    "### Import Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ba6732-93d0-47d7-9a02-a99a655517cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ./mnist1d_data.pkl\n"
     ]
    }
   ],
   "source": [
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c6c97-3ef6-41d0-b3a1-8f24700fd66c",
   "metadata": {},
   "source": [
    "### Define Training and Validation Data\n",
    "Separate the input data into training and validation data and store it in their respective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e8383c-1470-4b39-b562-425047512f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = data['x'].transpose()\n",
    "train_data_y = data['y']\n",
    "val_data_x = data['x_test'].transpose()\n",
    "val_data_y = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003c512-a761-4581-910a-3bc22af74503",
   "metadata": {},
   "source": [
    "### Print Dimensions of Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26b3fed-0f97-493c-8f34-2de964ff5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 4000 examples (columns), each of which has 40 dimensions (rows)\n",
      "Validation data: 4000 examples (columns), each of which has 40 dimensions (rows)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data: %d examples (columns), each of which has %d dimensions (rows)\"%((train_data_x.shape[1],train_data_x.shape[0])))\n",
    "print(\"Validation data: %d examples (columns), each of which has %d dimensions (rows)\"%((val_data_x.shape[1],val_data_x.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93d710-5e1a-4b58-aecd-ef374e68da0d",
   "metadata": {},
   "source": [
    "### Define Variance Function\n",
    "Define a function that computes the variance between the hidden unit outputs per hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8c85b1-a046-4b60-9887-7124f9f43062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_variance(name, data):\n",
    "  # First dimension (rows) is batch elements (# of input data)\n",
    "  # Second dimension (columns) is number of neurons.\n",
    "  np_data = data.detach().numpy()\n",
    "  # Compute variance across neurons and average these variances over members of the batch\n",
    "  neuron_variance = np.mean(np.var(np_data, axis=0))\n",
    "  # Print out the name and the variance\n",
    "  print(\"%s variance=%f\"%(name,neuron_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413af0d1-9b1b-4552-9389-c5975e9a1ad8",
   "metadata": {},
   "source": [
    "### Define He Initialization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353c05fc-50e2-4c09-9413-fcbc62a4b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560a972-9cf8-4dbb-9b98-828ddb00cf40",
   "metadata": {},
   "source": [
    "### Define Backpropagation Function\n",
    "Define a function that computes the backward pass and updates the parameter of the model based on the computed gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25cc9769-1c0d-42df-958c-a13413df1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_step_of_model(model, x_train, y_train):\n",
    "  # Define cross entropy loss function as the loss function for the residual neural network\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Define the stochastic gradient descent step and initialize learning rate and momentum\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
    "\n",
    "  # Load the converted data into a class that creates the batches\n",
    "  data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=200, shuffle=True, worker_init_fn=np.random.seed(1))\n",
    "\n",
    "  # Initialize model weights\n",
    "  model.apply(weights_init)\n",
    "\n",
    "  # For each example in the input data\n",
    "  for i, data in enumerate(data_loader):\n",
    "    # Retrieve inputs and labels for this batch (example)\n",
    "    x_batch, y_batch = data\n",
    "    # Reset the parameter gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Compute the forward pass and the model output\n",
    "    pred = model(x_batch)\n",
    "    # Compute the loss\n",
    "    loss = loss_function(pred, y_batch)\n",
    "    # Compute the backward pass\n",
    "    loss.backward()\n",
    "    # Undergo the SGD update\n",
    "    optimizer.step()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eaf459-a0a2-44fe-97a2-407ef830194e",
   "metadata": {},
   "source": [
    "### Format Training and Test Data\n",
    "Convert the training and test data into proper format for training the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a68198-4061-4c37-9f33-272ca049f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(train_data_x.transpose().astype('float32'))\n",
    "y_train = torch.tensor(train_data_y.astype('long'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a34f78-1fda-4b5e-94eb-2147c2a7c1f7",
   "metadata": {},
   "source": [
    "### Define Residual Neural Network\n",
    "Define a residual neural network model with 5 residual branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a7130a-3876-4745-9b51-04b720b16a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_size, output_size, hidden_size=100):\n",
    "    super(ResidualNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "    self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear5 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear6 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear7 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def count_params(self):\n",
    "    return sum([p.view(-1).shape[0] for p in self.parameters()])\n",
    "\n",
    "  def forward(self, x):\n",
    "    print_variance(\"Input\",x)\n",
    "    f = self.linear1(x)\n",
    "    print_variance(\"First preactivation\",f)\n",
    "    res1 = f+ self.linear2(f.relu())\n",
    "    print_variance(\"After first residual connection\",res1)\n",
    "    res2 = res1 + self.linear3(res1.relu())\n",
    "    print_variance(\"After second residual connection\",res2)\n",
    "    res3 = res2 + self.linear4(res2.relu())\n",
    "    print_variance(\"After third residual connection\",res3)\n",
    "    res4 = res3 + self.linear5(res3.relu())\n",
    "    print_variance(\"After fourth residual connection\",res4)\n",
    "    res5 = res4 + self.linear6(res4.relu())\n",
    "    print_variance(\"After fifth residual connection\",res5)\n",
    "    return self.linear7(res5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd409a-e4e2-4627-baf4-bee655a4dd00",
   "metadata": {},
   "source": [
    "### Initialize Hyperparameters\n",
    "Initialize hyperparameters for the residual neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61d9d9e-3a06-4514-b546-3e7b21f6be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "n_input = 40\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bda9ea-7511-46fc-be96-093acd8f3f5f",
   "metadata": {},
   "source": [
    "### Define Residual Neural Network Model\n",
    "Define the residual neural network model using the defined hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effca815-e5f8-43ba-a15a-6eedcde00d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualNetwork(n_input, n_output, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a77d85-744e-4750-ab67-95d74ad64fc7",
   "metadata": {},
   "source": [
    "### Compute Variance\n",
    "For each backward pass, undergo backpropagation and determine the variance between the hidden unit outputs for each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26957b4f-f583-4f33-b732-b6ff2c6b54ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input variance=1.055562\n",
      "First preactivation variance=2.071517\n",
      "After first residual connection variance=3.586905\n",
      "After second residual connection variance=6.299599\n",
      "After third residual connection variance=11.865705\n",
      "After fourth residual connection variance=21.049721\n",
      "After fifth residual connection variance=39.599545\n"
     ]
    }
   ],
   "source": [
    "run_one_step_of_model(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b590f9-456c-48fc-859a-658fd045465f",
   "metadata": {},
   "source": [
    "### Define Residual Neural Network Model with Batch Normalization\n",
    "Define a residual neural network model that undergoes batch normalization before each pre-activation computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a32072-7e66-42b1-a2b8-ae70af9aa6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNetworkWithBatchNorm(torch.nn.Module):\n",
    "  def __init__(self, input_size, output_size, hidden_size=100):\n",
    "    super(ResidualNetworkWithBatchNorm, self).__init__()\n",
    "    self.batchnorm1 = nn.BatchNorm1d(input_size)\n",
    "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "    self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.batchnorm3 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.batchnorm4 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.batchnorm5 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear5 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.batchnorm6 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear6 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.batchnorm7 = nn.BatchNorm1d(hidden_size)\n",
    "    self.linear7 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def count_params(self):\n",
    "    return sum([p.view(-1).shape[0] for p in self.parameters()])\n",
    "\n",
    "  def forward(self, x):\n",
    "    print_variance(\"Input\",x)\n",
    "    x = self.batchnorm1(x)\n",
    "    f = self.linear1(x)\n",
    "    print_variance(\"First preactivation\",f)\n",
    "    f = self.batchnorm2(f)\n",
    "    res1 = f+ self.linear2(f.relu())\n",
    "    print_variance(\"After first residual connection\",res1)\n",
    "    res1 = self.batchnorm3(res1)\n",
    "    res2 = res1 + self.linear3(res1.relu())\n",
    "    print_variance(\"After second residual connection\",res2)\n",
    "    res2 = self.batchnorm4(res2)\n",
    "    res3 = res2 + self.linear4(res2.relu())\n",
    "    print_variance(\"After third residual connection\",res3)\n",
    "    res3 = self.batchnorm5(res3)\n",
    "    res4 = res3 + self.linear5(res3.relu())\n",
    "    print_variance(\"After fourth residual connection\",res4)\n",
    "    res4 = self.batchnorm6(res4)\n",
    "    res5 = res4 + self.linear6(res4.relu())\n",
    "    res5 = self.batchnorm7(res5)\n",
    "    print_variance(\"After fifth residual connection\",res5)\n",
    "    return self.linear7(res5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b4b14-29c0-4b4d-b66f-05434a2125c2",
   "metadata": {},
   "source": [
    "### Initialize Hyperparameters\n",
    "Initialize hyperparameters for the residual neural network with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb25a177-698c-4c08-b0d6-a925c58d5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "n_input = 40\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1ba76-61be-4d50-99fd-59ada79a95e8",
   "metadata": {},
   "source": [
    "### Define Residual Neural Network Model with Batch Normalization\n",
    "Define the residual neural network model with batch normalization using the defined hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a187649-1460-4d92-9d55-a7e49cdf1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualNetworkWithBatchNorm(n_input, n_output, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7f7b2-9b72-4b33-8085-eefa5447f515",
   "metadata": {},
   "source": [
    "### Compute Variance\n",
    "For each backward pass, undergo backpropagation and determine the variance between the hidden unit outputs for each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3723de5-1372-44e5-b558-4e4b49042fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input variance=1.031035\n",
      "First preactivation variance=1.906459\n",
      "After first residual connection variance=1.671842\n",
      "After second residual connection variance=1.673384\n",
      "After third residual connection variance=1.740552\n",
      "After fourth residual connection variance=1.689274\n",
      "After fifth residual connection variance=0.999994\n"
     ]
    }
   ],
   "source": [
    "run_one_step_of_model(model, x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
