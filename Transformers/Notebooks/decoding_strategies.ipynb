{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e397f3c-d214-4e9b-927b-1826c888f1da",
   "metadata": {},
   "source": [
    "# Decoding Strategies\n",
    "This notebook aims to better understanding decoding strategies by importing a transformer model to determine a probability distribution of possible tokens to complete or continue the sentence. Each decoding strategy will choose a token from the probability distribution of possible tokens in different ways, leading to different completions of the same initial text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3d51b-c8b7-4708-9662-b4f28e5814fa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504235e9-83fb-4565-be86-6696d57354cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d93b79-a56d-40a1-8a9a-36023679b779",
   "metadata": {},
   "source": [
    "### Print Tokenizer Dictionary\n",
    "Print 20 different tokens that are defined in the dictionary of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247a913d-3e7b-4104-b388-1881767648a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary = 50257\n",
      "Token: 33003  Mormons\n",
      "Token: 12172  cam\n",
      "Token: 5192  trig\n",
      "Token: 32511 ojure\n",
      "Token: 50057  gist\n",
      "Token: 43723  Petition\n",
      "Token: 7813  sin\n",
      "Token: 21440  Witness\n",
      "Token: 32912  Remy\n",
      "Token: 20609 isure\n",
      "Token: 49100  creeps\n",
      "Token: 7751  fasc\n",
      "Token: 43757  Alc\n",
      "Token: 31228  messenger\n",
      "Token: 36230  SYSTEM\n",
      "Token: 32025  precipitation\n",
      "Token: 21758  cores\n",
      "Token: 45413  Forestry\n",
      "Token: 35730  guru\n",
      "Token: 8444  Disc\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(\"Number of tokens in dictionary = %d\"%(tokenizer.vocab_size))\n",
    "for i in range(20):\n",
    "  index = np.random.randint(tokenizer.vocab_size)\n",
    "  print(\"Token: %d \"%(index)+tokenizer.decode(torch.tensor(index), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d8f8c-79ff-42b0-8998-7bd31298e479",
   "metadata": {},
   "source": [
    "### Define Sampling Function\n",
    "Define sampling, which randomly chooses a token from the model's probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9eb30cf-9e42-46e0-91bd-e9aaa89d8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(input_tokens, model, tokenizer):\n",
    "  # Run the transformer model to get the prediction over the next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Compute the probabilities of the prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "  # Choose random token according to the probabilities\n",
    "  next_token = [np.random.choice(tokenizer.vocab_size, p = prob_over_tokens)]\n",
    "\n",
    "  # Append chosen token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446c1e0-74b9-4858-99a8-218d3c3fb221",
   "metadata": {},
   "source": [
    "### Define Input Text\n",
    "Define an input text for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffcb829-cca4-4624-b0df-0c2517b39dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f49e5f-2cf8-453d-95b6-7d04bd69b6ef",
   "metadata": {},
   "source": [
    "### Compute Input Tokens\n",
    "Convert the input text to tokens using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87aea508-41e1-407c-99ce-590e2b1ee978",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040e2e1-a7af-46b1-b624-e3db823b3fcb",
   "metadata": {},
   "source": [
    "### Complete Sentence with Sampling\n",
    "Run the model using sample_next_token to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caac9918-74ff-4399-a850-452316950c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that they\n",
      "The best thing about Bath is that they don\n",
      "The best thing about Bath is that they don't\n",
      "The best thing about Bath is that they don't even\n",
      "The best thing about Bath is that they don't even change\n",
      "The best thing about Bath is that they don't even change or\n",
      "The best thing about Bath is that they don't even change or shrink\n",
      "The best thing about Bath is that they don't even change or shrink anymore\n",
      "The best thing about Bath is that they don't even change or shrink anymore.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = sample_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891ea40-9e38-43ed-8464-9b43c6ac7197",
   "metadata": {},
   "source": [
    "### Define Greedy Token Selection\n",
    "Define greedy token selection, which chooses the token with the highest probability from the model's probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe3fd44-f3ec-4657-95ac-22e2337711b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_next_token(input_tokens, model, tokenizer):\n",
    "  # Run the transformer model to get the prediction over the next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Compute the probabilities of the prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "  # Compute the token index with the maximum probability\n",
    "  next_token = [np.argmax(prob_over_tokens)]\n",
    "\n",
    "  # Append chosen token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1545e-1514-4c20-8676-e6145f6c42a0",
   "metadata": {},
   "source": [
    "### Define Input Text\n",
    "Define input text for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452967bc-2338-4104-ae22-328c6445a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3dab75-b9ef-4cc0-b6e0-258859b59cb1",
   "metadata": {},
   "source": [
    "### Compute Input Tokens\n",
    "Convert the input text into tokens using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c246b4-7cf2-43fe-8dde-73a9e539b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97926fa-3811-4333-b00a-deb7c78c58a3",
   "metadata": {},
   "source": [
    "### Complete Sentence with Greedy Token Selection\n",
    "Run the model using get_best_next_token to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b27197e9-2646-4ded-a73a-d8931f300a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that it\n",
      "The best thing about Bath is that it's\n",
      "The best thing about Bath is that it's a\n",
      "The best thing about Bath is that it's a place\n",
      "The best thing about Bath is that it's a place where\n",
      "The best thing about Bath is that it's a place where you\n",
      "The best thing about Bath is that it's a place where you can\n",
      "The best thing about Bath is that it's a place where you can go\n",
      "The best thing about Bath is that it's a place where you can go to\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = get_best_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2138950-5b45-42e1-960c-204d394e02d7",
   "metadata": {},
   "source": [
    "### Define Top-K sampling\n",
    "Define top-k sampling, which randomly chooses the top K most probable tokens from the model's probability distribution as the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceedadce-0b32-4960-b6d4-3e232e9967fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_token(input_tokens, model, tokenizer, k=20):\n",
    "  # Run the transformer model to get the prediction over the next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Compute the probabilities of the prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Sort the probabilities from largest to smallest\n",
    "  sorted_prob_over_tokens =  np.sort(prob_over_tokens)[::-1]\n",
    "\n",
    "  # Find the probability at the k'th position\n",
    "  kth_prob_value = sorted_prob_over_tokens[k]\n",
    "\n",
    "  # Set all probabilities below the k'th value to zero\n",
    "  prob_over_tokens[prob_over_tokens<kth_prob_value] = 0\n",
    "\n",
    "  # Renormalize the non-zero probabilities so that they sum to one\n",
    "  prob_over_tokens = prob_over_tokens/np.sum(prob_over_tokens)\n",
    "\n",
    "  # Draw random token\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c8036-9fa3-4d6c-99c8-78c473971059",
   "metadata": {},
   "source": [
    "### Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef3bf3a-02d7-4f7f-be47-ccba52124f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938899e-3d09-4b26-be21-e0d95aea2458",
   "metadata": {},
   "source": [
    "### Compute Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6fa4a58-08fb-4f97-84f3-5fbe8b13c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d2c2f-58a3-4d6f-ab13-0b7d99f9c793",
   "metadata": {},
   "source": [
    "### Complete Sentence using Top-K Sampling\n",
    "Run the model using get_top_k_token to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b18bec6-7cbf-45d6-9005-95425cc50186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hd/f_gk36z51lj__phqtmgrmv8w0000gn/T/ipykernel_59804/4291433395.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that you\n",
      "The best thing about Bath is that you get\n",
      "The best thing about Bath is that you get to\n",
      "The best thing about Bath is that you get to see\n",
      "The best thing about Bath is that you get to see all\n",
      "The best thing about Bath is that you get to see all the\n",
      "The best thing about Bath is that you get to see all the beautiful\n",
      "The best thing about Bath is that you get to see all the beautiful faces\n",
      "The best thing about Bath is that you get to see all the beautiful faces of\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = get_top_k_token(input_tokens, model, tokenizer, k=10)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35499922-56e7-4607-bb96-b7444528303f",
   "metadata": {},
   "source": [
    "### Define Nucleus Sampling\n",
    "Define nucleus sampling, which randomly chooses a token from a list of sorted tokens whose cumulative sum doesn't exceed a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf61e99-92ee-4589-ace8-466726437bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh=0.25):\n",
    "  # Run the transformer model to get the prediction over the next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Compute the probabilities of the prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Sort the probabilities in decreasing order\n",
    "  sorted_probs_decreasing = np.sort(prob_over_tokens)[::-1]\n",
    "\n",
    "  # Compute the cumulative sum of these probabilities\n",
    "  cum_sum_probs = np.cumsum(sorted_probs_decreasing)\n",
    "\n",
    "  # Find index where that the cumulative sum is greater than the threshold\n",
    "  thresh_index = np.argmax(cum_sum_probs>thresh)\n",
    "  print(\"Choosing from %d tokens\"%(thresh_index))\n",
    "\n",
    "  # Compute the probability value at the tresh_index\n",
    "  thresh_prob = sorted_probs_decreasing[thresh_index]\n",
    "\n",
    "  # Set any probabilities below the tresh_prob to zero\n",
    "  prob_over_tokens[prob_over_tokens<thresh_prob] = 0\n",
    "\n",
    "  # Renormalize the probabilities to sum to 1\n",
    "  prob_over_tokens = prob_over_tokens / np.sum(prob_over_tokens)\n",
    "\n",
    "  # Draw a random token\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03922e28-ea38-424e-860c-d20395187ed1",
   "metadata": {},
   "source": [
    "### Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11dcf73b-0cbc-4b8e-90c5-d7afc254e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800e9b-fd44-4abd-9c05-f699eb90f2c1",
   "metadata": {},
   "source": [
    "### Compute Input Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2993e89-bd77-4b35-9809-2b433dddda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af8b9b-7dc8-45f0-a5c2-fcf7f69b2de8",
   "metadata": {},
   "source": [
    "### Complete Sentence using Nucleus Sampling\n",
    "Run the model using get_nucleus_sampling_token to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e216b6-e3cb-484e-b864-cc6b86b23249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that\n",
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that it\n",
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that it's\n",
      "Choosing from 2 tokens\n",
      "The best thing about Bath is that it's not\n",
      "Choosing from 1 tokens\n",
      "The best thing about Bath is that it's not a\n",
      "Choosing from 25 tokens\n",
      "The best thing about Bath is that it's not a city\n",
      "Choosing from 2 tokens\n",
      "The best thing about Bath is that it's not a city that\n",
      "Choosing from 1 tokens\n",
      "The best thing about Bath is that it's not a city that has\n",
      "Choosing from 1 tokens\n",
      "The best thing about Bath is that it's not a city that has been\n",
      "Choosing from 11 tokens\n",
      "The best thing about Bath is that it's not a city that has been around\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh = 0.2)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d824f3d-950c-4faa-8ecc-b5c81c8dbe62",
   "metadata": {},
   "source": [
    "### Define K Sampling\n",
    "Define K sampling, which returns the k'th most likely next token from the model's probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbd22d5e-c530-4a58-b3ee-fbb873af9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kth_most_likely_token(input_tokens, model, tokenizer, k):\n",
    "  # Run the transformer model to get the prediction over the next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Compute the probabilities of the prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Sort the probabilities from largest to smallest\n",
    "  sorted_prob_over_tokens = np.sort(prob_over_tokens)[::-1]\n",
    "\n",
    "  # Find the k'th sorted probability\n",
    "  kth_prob_value = sorted_prob_over_tokens[k]\n",
    "\n",
    "  # Locate the position of the token with the k'th probability\n",
    "  next_token = np.where(prob_over_tokens == kth_prob_value)[0]\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  output_tokens['log_prob'] = output_tokens['log_prob'] + np.log(prob_over_tokens[next_token])\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ff981-b6e2-424f-b5af-f6716efb58dd",
   "metadata": {},
   "source": [
    "### Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df52f7cd-8cb7-462a-9408-ebdfe1e0f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5dc842-bb2a-48fc-8c3e-4697629dc706",
   "metadata": {},
   "source": [
    "### Compute Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e6d2a8-8e3e-4158-a6f5-e03ac6ee8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bf102-a811-47cd-bdfe-f53d0d72a870",
   "metadata": {},
   "source": [
    "### Complete Sentence with K Sampling (k = 1)\n",
    "Run the model using get_kth_most_likely_token, where k = 1, to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d69d4c-88fd-4050-ae39-cae726f961d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is the\n",
      "The best thing about Bath is the way\n",
      "The best thing about Bath is the way you\n",
      "The best thing about Bath is the way you get\n",
      "The best thing about Bath is the way you get the\n",
      "The best thing about Bath is the way you get the most\n",
      "The best thing about Bath is the way you get the most bang\n",
      "The best thing about Bath is the way you get the most bang out\n",
      "The best thing about Bath is the way you get the most bang outta\n",
      "The best thing about Bath is the way you get the most bang outta the\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=1)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a1cfc-2e5c-4115-a6e9-48bcf369aaac",
   "metadata": {},
   "source": [
    "### Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96320bb4-d022-4029-b8ee-2b8156fbea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c482416-1c21-4ded-9f2a-d66b68677e9a",
   "metadata": {},
   "source": [
    "### Compute Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ed4ba1a-95f3-412b-9b14-948c0eef962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c1cb7-f60f-49c5-86b5-135f2266d4d6",
   "metadata": {},
   "source": [
    "### Complete Sentence with K Sampling (k = 2000)\n",
    "Run the model using get_kth_most_likely_token, where k = 2000, to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff1c3dd3-46e1-49ad-bd15-8c4ee391ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is mixed\n",
      "The best thing about Bath is mixed profits\n",
      "The best thing about Bath is mixed profits partnerships\n",
      "The best thing about Bath is mixed profits partnerships»\n",
      "The best thing about Bath is mixed profits partnerships» buy\n",
      "The best thing about Bath is mixed profits partnerships» buy generic\n",
      "The best thing about Bath is mixed profits partnerships» buy generic+\n",
      "The best thing about Bath is mixed profits partnerships» buy generic+drive\n",
      "The best thing about Bath is mixed profits partnerships» buy generic+drive outlets\n",
      "The best thing about Bath is mixed profits partnerships» buy generic+drive outlets concentrate\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=2000)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ae6a7-9f3f-47e1-931e-1e8455089a01",
   "metadata": {},
   "source": [
    "### Define Print Beams Function\n",
    "Define a function that prints each beam along with its log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a7d6cdf-ea23-4ceb-b3a1-1c0a73b90be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_beams(beams):\n",
    "  for index,beam in enumerate(beams):\n",
    "    print(\"Beam %d, Prob %3.3f: \"%(index,beam['log_prob'])+tokenizer.decode(beam[\"input_ids\"][0], skip_special_tokens=True))\n",
    "  print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e3ef0-f06d-403f-8691-35c4cc4f6233",
   "metadata": {},
   "source": [
    "### Define Beam Search\n",
    "Define beam search, which computes n_beams most likely tokens as its initial beams, makes possible continuation of these beams, and keeps only the top n_beams beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b94d381b-f531-4546-a62b-62f6989b03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_beam_search(input_tokens_in, model, tokenizer, n_beam=5, beam_length=10):\n",
    "  # Store beams in a list\n",
    "  input_tokens['log_prob'] = 0.0\n",
    "\n",
    "  # Initialize the n_beams most likely tokens as the initial beam\n",
    "  beams = [None] * n_beam\n",
    "  for c_k in range(n_beam):\n",
    "    beams[c_k] = dict(input_tokens_in)\n",
    "    beams[c_k] = get_kth_most_likely_token(beams[c_k], model, tokenizer, c_k)\n",
    "\n",
    "  print_beams(beams)\n",
    "\n",
    "  # For each token in the sequence we will add\n",
    "  for c_pos in range(beam_length-1):\n",
    "    # For each computed beam, initialize the n_beams most likely tokens as possible continuations of the beam\n",
    "    beams_all = [None] * (n_beam*n_beam)\n",
    "    log_probs_all = np.zeros(n_beam*n_beam)\n",
    "    # For each current hypothesis\n",
    "    for c_beam in range(n_beam):\n",
    "      # For each continuation\n",
    "      for c_k in range(n_beam):\n",
    "        # Store the continuation and the probability\n",
    "        beams_all[c_beam * n_beam + c_k] = dict(get_kth_most_likely_token(beams[c_beam], model, tokenizer, c_k))\n",
    "        log_probs_all[c_beam * n_beam + c_k] = beams_all[c_beam * n_beam + c_k]['log_prob']\n",
    "\n",
    "    # Keep the best n_beams sequences with the highest probabilities\n",
    "    sorted_index = np.argsort(np.array(log_probs_all)*-1)\n",
    "    for c_k in range(n_beam):\n",
    "      beams[c_k] = dict(beams_all[sorted_index[c_k]])\n",
    "\n",
    "    # Print the beams\n",
    "    print_beams(beams)\n",
    "\n",
    "  return beams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22a524-abf5-4665-8372-e27582096cac",
   "metadata": {},
   "source": [
    "### Define Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1441a75d-2451-44f3-b5e0-01148cb647ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9579a-921f-4005-964c-848fe83b5667",
   "metadata": {},
   "source": [
    "### Compute Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99fdcb72-8d8f-4c0d-8707-e3de81d6a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94335082-72c8-4329-9f41-d05fe5716b73",
   "metadata": {},
   "source": [
    "### Complete Sentence with Beam Search\n",
    "Run the model using do_beam_search to observe how the sentence is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d20e518e-5c3f-4a25-853e-da94b4d44218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hd/f_gk36z51lj__phqtmgrmv8w0000gn/T/ipykernel_59804/610459643.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Beam %d, Prob %3.3f: \"%(index,beam['log_prob'])+tokenizer.decode(beam[\"input_ids\"][0], skip_special_tokens=True))\n",
      "/var/folders/hd/f_gk36z51lj__phqtmgrmv8w0000gn/T/ipykernel_59804/3700701688.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  log_probs_all[c_beam * n_beam + c_k] = beams_all[c_beam * n_beam + c_k]['log_prob']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam 0, Prob -0.727: The best thing about Bath is that\n",
      "Beam 1, Prob -2.161: The best thing about Bath is the\n",
      "Beam 2, Prob -3.177: The best thing about Bath is it\n",
      "Beam 3, Prob -3.468: The best thing about Bath is how\n",
      "Beam 4, Prob -3.536: The best thing about Bath is you\n",
      "---\n",
      "Beam 0, Prob -1.899: The best thing about Bath is that it\n",
      "Beam 1, Prob -3.931: The best thing about Bath is it's\n",
      "Beam 2, Prob -4.079: The best thing about Bath is that it is\n",
      "Beam 3, Prob -4.433: The best thing about Bath is the fact\n",
      "Beam 4, Prob -4.553: The best thing about Bath is you can\n",
      "---\n",
      "Beam 0, Prob -2.740: The best thing about Bath is that it's\n",
      "Beam 1, Prob -4.657: The best thing about Bath is the fact that\n",
      "Beam 2, Prob -5.331: The best thing about Bath is that it's not\n",
      "Beam 3, Prob -6.227: The best thing about Bath is it's a\n",
      "Beam 4, Prob -6.264: The best thing about Bath is that it is a\n",
      "---\n",
      "Beam 0, Prob -4.938: The best thing about Bath is that it's a\n",
      "Beam 1, Prob -6.012: The best thing about Bath is the fact that it\n",
      "Beam 2, Prob -7.313: The best thing about Bath is that it's not a\n",
      "Beam 3, Prob -7.907: The best thing about Bath is the fact that it is\n",
      "Beam 4, Prob -8.110: The best thing about Bath is that it's a great\n",
      "---\n",
      "Beam 0, Prob -6.919: The best thing about Bath is the fact that it's\n",
      "Beam 1, Prob -7.764: The best thing about Bath is that it's a place\n",
      "Beam 2, Prob -8.930: The best thing about Bath is that it's a great place\n",
      "Beam 3, Prob -9.479: The best thing about Bath is that it's a place that\n",
      "Beam 4, Prob -9.543: The best thing about Bath is the fact that it's not\n",
      "---\n",
      "Beam 0, Prob -8.451: The best thing about Bath is that it's a place where\n",
      "Beam 1, Prob -9.124: The best thing about Bath is the fact that it's a\n",
      "Beam 2, Prob -9.285: The best thing about Bath is that it's a great place to\n",
      "Beam 3, Prob -10.536: The best thing about Bath is that it's a place where people\n",
      "Beam 4, Prob -11.255: The best thing about Bath is that it's a place that's\n",
      "---\n",
      "Beam 0, Prob -9.306: The best thing about Bath is that it's a place where you\n",
      "Beam 1, Prob -11.327: The best thing about Bath is that it's a place where people can\n",
      "Beam 2, Prob -11.839: The best thing about Bath is that it's a great place to live\n",
      "Beam 3, Prob -12.030: The best thing about Bath is that it's a place where you don\n",
      "Beam 4, Prob -12.365: The best thing about Bath is the fact that it's a place\n",
      "---\n",
      "Beam 0, Prob -9.752: The best thing about Bath is that it's a place where you can\n",
      "Beam 1, Prob -12.031: The best thing about Bath is that it's a place where you don't\n",
      "Beam 2, Prob -12.629: The best thing about Bath is that it's a place where you can get\n",
      "Beam 3, Prob -13.006: The best thing about Bath is that it's a great place to live.\n",
      "Beam 4, Prob -13.107: The best thing about Bath is the fact that it's a place where\n",
      "---\n",
      "Beam 0, Prob -12.273: The best thing about Bath is that it's a place where you can go\n",
      "Beam 1, Prob -12.603: The best thing about Bath is that it's a place where you don't have\n",
      "Beam 2, Prob -14.056: The best thing about Bath is the fact that it's a place where you\n",
      "Beam 3, Prob -14.324: The best thing about Bath is that it's a place where you can go and\n",
      "Beam 4, Prob -14.492: The best thing about Bath is that it's a great place to live. It\n",
      "---\n",
      "Beam 0, Prob -12.662: The best thing about Bath is that it's a place where you don't have to\n",
      "Beam 1, Prob -13.658: The best thing about Bath is that it's a place where you can go to\n",
      "Beam 2, Prob -14.469: The best thing about Bath is the fact that it's a place where you can\n",
      "Beam 3, Prob -14.785: The best thing about Bath is that it's a great place to live. It's\n",
      "Beam 4, Prob -14.896: The best thing about Bath is that it's a place where you don't have to be\n",
      "---\n",
      "Beam search result:\n",
      "The best thing about Bath is that it's a place where you don't have to\n"
     ]
    }
   ],
   "source": [
    "n_beams = 5\n",
    "best_beam = do_beam_search(input_tokens,model,tokenizer)\n",
    "print(\"Beam search result:\")\n",
    "print(tokenizer.decode(best_beam[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
