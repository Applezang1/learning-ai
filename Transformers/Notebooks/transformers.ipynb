{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e7642c-59ce-4730-a8b5-24fbbfe47e12",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "This notebook aims to help gain an understanding of transformers by defining the types of transformers and their functions. The notebook then goes in depth about the architecture of encoders and decoders and the mechanisms that makes up the encoders (examples include feed forward networks and multi-head self attention mechanims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fa0b2-9547-41fd-b412-208db793d4c2",
   "metadata": {},
   "source": [
    "## Types of Transformers\n",
    "### Encoder-Only Transformer \n",
    "<ins>Function</ins>: Encoder-only transformers take in an input sequence and contextualizes its meaning. It aims to understand the input sequence possibly for other tasks \n",
    "\n",
    "<ins>Use Cases</ins>: Classification of Input Data\n",
    "\n",
    "### Decoder-Only Transformer \n",
    "<ins>Function</ins>: Decoder-only transformers take in an output sequence generated by the decoder and uses the output sequence to formulate a continuation of the output sequence.  \n",
    "\n",
    "<ins>Use Cases</ins>: Text Completion \n",
    "\n",
    "### Encoder-Decoder Transformer \n",
    "<ins>Function</ins>: Encoder-decoder transformers contextualizes the meaning of an input sequence through an encoder, and uses the information to generate an separate output sequence. \n",
    "\n",
    "<ins>Use Cases</ins>: Language Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9f92a-092b-4f6c-a052-426394d2ae76",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Encoders are generally used to obtain contextualized information about the words/phrases in an input sequence. \n",
    "\n",
    "The figure below represents the structure of an **encoder**:\n",
    "<img src=\"EncoderImage.png\" width=\"250\" style=\"display:block; margin-left:auto; margin-right:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa0432-205a-46e3-88f0-00a4f90b2ad1",
   "metadata": {},
   "source": [
    "## Input Embedding \n",
    "<ins>Function</ins>: Input embedding aims to represent input tokens as vectors in a high-dimensional space. \n",
    "\n",
    "#### Tokens \n",
    "For a transformer, the input is split into **tokens** using a **tokenizer**. Tokens can be represented as complete words, phrases of words, or punctuations.  \n",
    "\n",
    "#### Input ID \n",
    "Afterwards, each token is assigned its unique **input ID**, which is based on the transformer's defined vocabulary. \n",
    "\n",
    "#### Embedding \n",
    "The input IDs of the transformer are mapped into vectors in a high-dimensional space, where the position of the vector in the high-dimensional space corresponds to the 'meaning' of the token. This is called an **embedding**, where a column represents a dimension of the embedding vector. \n",
    "\n",
    "The values representing the vectors of each token is a parameter, which is learned and adjusted during training to better represent the 'meaning' of the token. The collection of the embeddings of each token are called the **embedding matrix**, where a column represents a dimension of all the tokens and a row represents a token ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff7684-5f85-40aa-bf2d-913011d22efc",
   "metadata": {},
   "source": [
    "## Positional Encoding \n",
    "<ins>Function</ins>: Positional encoding aims to represent a token's relationship to neighboring tokens into the vector of the token. \n",
    "\n",
    "### Positional Embedding \n",
    "Taking the input tokens and computing the following equations outputs the **positional embedding**, which represents information obtained from a token's relationship to neighboring tokens. The equation is as follows:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin \\left( \\frac{pos}{10000^{2i/d}} \\right)   \\text{ (even-embedding dimensions)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos \\left( \\frac{pos}{10000^{2i/d}} \\right)   \\text{ (odd-embedding dimensions)}\n",
    "$$\n",
    "where:\n",
    "- $pos$ = token position (0-indexed, start with 0)  \n",
    "- $i$ = embedding dimension index  \n",
    "- $d$ = embedded vector dimension\n",
    "\n",
    "Essentially, the positional embedding is calculated by inputting the position of the token in respect to the other tokens ($pos$) and a dimension of the token ($i$) into the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4605b3-924a-488e-a39d-427aa284a9ea",
   "metadata": {},
   "source": [
    "## Finalized Embedding \n",
    "The positional embedding and input embedding are combined to form the **encoder input**, where a column represents the dimensions of the encoder input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0565f08-fb4c-4c26-85e0-913de252087a",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "Self-attention methods are a crucial part of transformers because they controls how much each input vector contributes to the output. In order to use self-attention methods, the attention weights for each input vector must be computed. \n",
    "\n",
    "### Keys, Queries, Values\n",
    "In a transformer model, the input vector is linearly transformed to create three separate values. These values are computed through the following equations:\n",
    "$$\n",
    "\\mathbf{q}_n = \\boldsymbol{\\beta}_q + \\mathbf{\\Omega}_q \\mathbf{x}_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}_m = \\boldsymbol{\\beta}_k + \\mathbf{\\Omega}_k \\mathbf{x}_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_m = \\boldsymbol{\\beta}_v + \\mathbf{\\Omega}_v \\mathbf{x}_m\n",
    "$$\n",
    "Where: \n",
    "- $\\mathbf{q}_n$ is defined as the **query** (dimension: token length, embedding dimension)\n",
    "- $\\mathbf{k}_m$ is defined as the **key** (dimension: token length, embedding dimension)\n",
    "- $\\mathbf{v}_m$ is defined as the **value**\n",
    "\n",
    "In the equation, $\\boldsymbol{\\beta}$ and $\\mathbf{\\Omega}$ are learnable parameters. Intuitively, keys represent the categories of the token, values represent the token, and the queries represent the inputted category\n",
    "\n",
    "### Attention Weights\n",
    "The query ($\\mathbf{q}_n$) and the key ($\\mathbf{k}_m$) are both used to compute the attention weights for the input vector. The attention weights are computed through the following equation:\n",
    "$$\n",
    "a[\\mathbf{x}_m, \\mathbf{x}_n] \n",
    "= \\text{softmax}_m\\left[ \\frac{\\mathbf{k}_m^{T} \\mathbf{q}_n }{\\sqrt{d_k}} \\right]\n",
    "$$\n",
    "Where: \n",
    "- $\\sqrt{d_k}$ is the embedding dimension (stabilizes term)\n",
    "\n",
    "The softmax function can also be presented as:\n",
    "$$\n",
    "= \\frac{\\exp\\left( \\mathbf{k}_m^{T} \\mathbf{q}_n \\right)}\n",
    "{\\sum_{m'=1}^{N} \\exp\\left( \\mathbf{k}_{m'}^{T} \\mathbf{q}_n \\right)}\n",
    "$$ \n",
    "\n",
    "In the equation, the output of the dot product between the query ($\\mathbf{q}_n$) and the key ($\\mathbf{k}_m$) are passed through the softmax function, creating the **attention weights** needed for the self-attention methods. (Note that the $\\sqrt{d_k}$ is still present)\n",
    "\n",
    "<ins>Definition of Attention Weight</ins>: The attention weights is a matrix with dimensions (token length, token length), where the intersecting values between two different tokens represents the relationship between the tokens. The intersection values between two of the same tokens is the highest because they are most closely related. \n",
    "\n",
    "### Self-Attention Outputs\n",
    "The **self-attention output** is computed by multiplying each attention weight by its corresponding value ($\\mathbf{v}_m$). The equation for the self-attention output is represented as:\n",
    "$$\n",
    "\\mathrm{sa}_n[\\mathbf{x}_1, \\ldots, \\mathbf{x}_N]\n",
    "= \\sum_{m=1}^{N} a[\\mathbf{x}_m, \\mathbf{x}_n] \\, \\mathbf{v}_m\n",
    "$$\n",
    "Where:\n",
    "- $a[\\mathbf{x}_m, \\mathbf{x}_n]$ is the computed attention weight\n",
    "- $\\mathbf{v}_m$ is the corresponding value\n",
    "- $\\sum_{m=1}^{N}$ represents the summation of the weighted sums from 1 to N\n",
    "\n",
    "This outputs the self-attention output, with dimensions (tokens, dimensions of embedding). In theory, the self-attention vector output is encoded with information from the input embedding, positional embedding, and attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adb26b-7ec2-41f4-8a56-d53c9b84c93a",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "In multi-head self-attention, the computed keys, queries, and values are split into smaller pieces on the embedding dimension. These pieces are defined as heads, and each will contain parts of embeddings from all the tokens\n",
    "\n",
    "For each head, the attention weight is computed using the same formula defined above. The individual self-attention outputs are used to compute the multi-head self-attention weight through the following formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{MhSa}[\\mathbf{X}] \n",
    "= \n",
    "\\mathbf{\\Omega}_c \n",
    "\\begin{bmatrix}\n",
    "\\mathrm{Sa}_1[\\mathbf{X}]^{T},\\;\n",
    "\\mathrm{Sa}_2[\\mathbf{X}]^{T},\\;\n",
    "\\dots,\\;\n",
    "\\mathrm{Sa}_H[\\mathbf{X}]^{T}\n",
    "\\end{bmatrix}^{T}.\n",
    "$$\n",
    "Where: \n",
    "- $\\mathrm{Sa}_1[\\mathbf{X}]^{T}$ represents the first self-attention output (first \"head\")\n",
    "- $\\mathbf{\\Omega}_c$ represents a weight value (parameter), which linearly transforms the multi-head self-attention output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f15da-e58a-455e-b2af-dac0cdaf4e43",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "Layer normalization normalizes the embeddings of a token for numerical stability and a stable scale. The formula for Layer Normalization is:\n",
    "\n",
    "$$\n",
    "\\mathrm{LN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $\\mathbf{x} = [x_1, x_2, \\dots, x_d]$ is the input vector (for a single token)  \n",
    "- $\\mu$ is the mean of the components of $\\mathbf{x}$, which is calculated by:  \n",
    "$$\n",
    "\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i\n",
    "$$\n",
    "- $\\sigma^2$ is the variance of the components of $\\mathbf{x}$, which is calculated by:  \n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "$$\n",
    "- $\\epsilon$ is a small constant added for numerical stability  \n",
    "- $\\boldsymbol{\\gamma}$ and $\\boldsymbol{\\beta}$ are learnable parameters (dimension $d$)  \n",
    "- $\\odot$ represents element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b868d-2b73-4fea-a3bc-313f1e32cce3",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "### Multi-Head Self Attention\n",
    "Using the multi-head self-attention mechanism, we are now able to define an encoder layer in the encoder. \n",
    "The input embeddings are used to compute the **multi-head self attention weights**, which are computed through:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathrm{MhSa}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathbf{X}$ represents the input of the encoder layer\n",
    "- $\\mathrm{MhSa}[\\mathbf{X}]$ represents the multi-head self-attention output\n",
    "\n",
    "### LayerNorm\n",
    "The computed multi-head self attention weights are then normalized through a **LayerNorm** using the following formula:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathrm{LayerNorm}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathrm{LayerNorm}$ stabilizes the X value by normalizing the mean and variance\n",
    "\n",
    "### Feed Foward Network\n",
    "After stabilizing the multi-head self-attention weights, the values are fed into a **feed forward network**, which is a fully connected nueral network:\n",
    "$$\n",
    "\\mathbf{x}_n \\leftarrow \\mathbf{x}_n + \\mathrm{mlp}[\\mathbf{x}_n] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathbf{x}_n$ represents a row from the vector $\\mathbf{X}$\n",
    "- $\\mathrm{mlp}[\\mathbf{x}_n]$ is a fully connected neural network that takes $\\mathbf{x}_n$ as its input\n",
    "\n",
    "Essentially, this equation is passing each computed $\\mathbf{X}$ value through a fully connected neural network and returning its output. \n",
    "\n",
    "### LayerNorm\n",
    "Finally, the output of the feed forward network is passed through a **LayerNorm** to normalize the output value's mean and variance. The equation is represented as:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathrm{LayerNorm}[\\mathbf{X}]\n",
    "$$\n",
    "\n",
    "**<ins>Note</ins>**: \n",
    "For equations \n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathrm{MhSa}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_n \\leftarrow \\mathbf{x}_n + \\mathrm{mlp}[\\mathbf{x}_n] \\\\\n",
    "$$\n",
    "\n",
    "Notice how $\\mathbf{X}$ is added back to the first equation and $\\mathbf{x}_n$ is added back to the second equation. These represent residual networks because they add the original input into the final output of the equation, allowing it to preserve past information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807bfcb-7e83-4bf6-87e4-08e920b5886f",
   "metadata": {},
   "source": [
    "## Encoder Output\n",
    "After the encoder processes the input embedding through multiple encoder layers, it outputs a matrix with dimensions of (tokens, embeddings per token). The outputted matrix consists of information from the input embedding, positional encoding, and the attention weights. Intuitively, the encoder output can be thought of as the contextualized version of the input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20226e37-78f3-402c-800d-2d94d8cffb3e",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "Decoders are generally used to formulate the next part of an output sequence given an output sequence generated by the decoder in a previous timestep. The figure below represents the structure of an **decoder**:\n",
    "<img src=\"DecoderImage.png\" width=\"250\" style=\"display:block; margin-left:auto; margin-right:auto;\">\n",
    "<ins>Note</ins>: In an Encoder-Decoder transformer, keys and values from the encoder is used as an input for the decoder's multi-head attention. In a Decoder-only transformer, the keys and values are generated from the input of the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fa479-39a4-4789-80bb-453cb711a289",
   "metadata": {},
   "source": [
    "## Masked Multi-Head Attention\n",
    "The decoder uses the same mechanisms such as multi-head attention and layer normalization. However, the decoder uses the **masked multi-head attention** in the beginning in contrast to the encoder, which uses normal multi-head attention. \n",
    "\n",
    "Masked multi-head attention calculates the attention weights by computing the dot product between the query and the key and stabilizing the term. This process is represented through the following equation.\n",
    "$$\n",
    "a[\\mathbf{x}_m, \\mathbf{x}_n] \n",
    "= \\text{softmax}_m\\left[ \\frac{\\mathbf{k}_m^{T} \\mathbf{q}_n }{\\sqrt{d_k}} \\right]\n",
    "$$\n",
    "Where: \n",
    "- $\\sqrt{d_k}$ is the embedding dimension (stabilizes term)\n",
    "\n",
    "However, masked multi-head attention ensures that the current token that it's calculating the attention weight for can only interact with previous tokens. Therefore for any computed attention weights for tokens past the current token, it replaces it with negative infinity. This ensures that when the attention weights are passed through the softmax function, the attention weights return 0 and prevent them from interacting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960548f-d7ae-4c1e-bbcd-f0cd31987191",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "Using the masked multi-head self-attention mechanism, we are now able to define a decoder layer in the decoder.\n",
    "\n",
    "### Masked Multi-Head Self Attention Weights\n",
    "The input embeddings are used to compute the **masked multi-head self-attention weights**, which are computed through the following formula:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathrm{MaSa}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathbf{X}$ represents the input of the decoder layer\n",
    "- $\\mathrm{MaSa}[\\mathbf{X}]$ represents the masked multi-head self-attention output\n",
    "\n",
    "### LayerNorm\n",
    "The computed weights are then normalized through **LayerNorm**, which is represented as:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathrm{LayerNorm}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathrm{LayerNorm}$ stabilizes the X value by normalizing the mean and variance\n",
    "\n",
    "### Multi-Head Self-Attention Weights\n",
    "After stabilizing the weights, the **multi-head self-attention weights** are computed:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathrm{MhSa}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "Where: \n",
    "- $\\mathbf{X}$ represents the input of the decoder layer\n",
    "- $\\mathrm{MhSa}[\\mathbf{X}]$ represents the multi-head self-attention output\n",
    "\n",
    "<ins>Note</ins>: The input of the mutli-head self attention depends on whether the transformer is an encoder-decoder or a decoder-only transformer. For an encoder-decoder transformer, the computed keys and values of the encoder is used as the input to the multi-head self attention of the decoder (Intuitvely, contextualized input embeddings are given to the decoder). For a decoder-only transformer, the keys and values comes from the decoder itself.\n",
    "\n",
    "### LayerNorm\n",
    "After computing the multi-head self attention weights, the weights are normalized again through **LayerNorm**, which is represented as follows: \n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathrm{LayerNorm}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "\n",
    "### Feed Forward Network\n",
    "After stabilizing the multi-head self attention weights, these values are inputted into a **feed forward network**. This is represented as: \n",
    "$$\n",
    "\\mathbf{x}_n \\leftarrow \\mathbf{x}_n + \\mathrm{mlp}[\\mathbf{x}_n] \\\\\n",
    "$$\n",
    "\n",
    "Where: \n",
    "- $\\mathbf{x}_n$ represents a row from the vector $\\mathbf{X}$\n",
    "- $\\mathrm{mlp}[\\mathbf{x}_n]$ is a fully connected neural network that takes $\\mathbf{x}_n$ as its input\n",
    "\n",
    "Essentially, this equation is passing each computed $\\mathbf{X}$ value through a fully connected neural network and returning its output. \n",
    "\n",
    "### LayerNorm\n",
    "Finally, the output of the fully connected neural network is passed through a **LayerNorm** to normalize the output value's mean and variance. The equation is represented as:\n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathrm{LayerNorm}[\\mathbf{X}]\n",
    "$$\n",
    "\n",
    "**<ins>Note</ins>**: \n",
    "For equations \n",
    "$$\n",
    "\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathrm{MhSa}[\\mathbf{X}] \\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_n \\leftarrow \\mathbf{x}_n + \\mathrm{mlp}[\\mathbf{x}_n] \\\\\n",
    "$$\n",
    "\n",
    "Notice how $\\mathbf{X}$ is added back to the first equation and $\\mathbf{x}_n$ is added back to the second equation. These represent residual networks because they add the original input into the final output of the equation, allowing it to preserve past information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e402dd-3c50-43dd-8eea-790d48c1dc96",
   "metadata": {},
   "source": [
    "## Decoder Output\n",
    "The decoder layer outputs a contextualized matrix of the input embeddings of the decoder, which contains input embeddings, positional encoding, masked self-attention weights, and encoder-decoder attention. The matrix is passed through a linear layer, which returns a score for all the possible continuations of the output sequence. These scores are passed through the softmax function and the token is selected based on the selection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f9501-4809-4965-aaca-f3f31cf23289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RDKit)",
   "language": "python",
   "name": "rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
